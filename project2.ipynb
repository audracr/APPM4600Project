{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c346c4-1698-496a-993d-752d865fa360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, time, warnings, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import solve, norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)   # silence exp overflow\n",
    "\n",
    "# ---------- random feature expansion -------------------------------------------\n",
    "def expand_features(X, k, seed=42, sparse=True):\n",
    "    \"\"\"30 features → 30·k via a random projection.  sparse keeps memory small.\"\"\"\n",
    "    if k == 1:\n",
    "        return X\n",
    "    rng  = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    D    = d * k\n",
    "    if sparse:\n",
    "        nnz = 3\n",
    "        P   = np.zeros((d, D))\n",
    "        for j in range(D):\n",
    "            rows            = rng.choice(d, nnz, replace=False)\n",
    "            P[rows,  j]     = rng.choice([-1, 1], nnz)\n",
    "    else:                       # dense Gaussian\n",
    "        P = rng.standard_normal((d, D))\n",
    "    return X @ P                # (n, D)\n",
    "\n",
    "def make_train_test(k=1):\n",
    "    \"\"\"return X_tr, X_te, y_tr, y_te    (bias column already added).\"\"\"\n",
    "    df = pd.read_csv(\"wdbc.data\", header=None)\n",
    "    df = df.drop(columns=0)\n",
    "    df[1] = df[1].map({'M':1, 'B':0})\n",
    "    X    = df.drop(columns=1).values\n",
    "    X    = expand_features(X, k)\n",
    "    y    = df[1].values.reshape(-1,1)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=10\n",
    "    )\n",
    "    sc   = StandardScaler()\n",
    "    X_tr = sc.fit_transform(X_tr); X_te = sc.transform(X_te)\n",
    "    X_tr = np.c_[np.ones(X_tr.shape[0]), X_tr]\n",
    "    X_te = np.c_[np.ones(X_te.shape[0]), X_te]\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "# ---------- logistic-loss pieces -----------------------------------------------\n",
    "sigmoid = lambda z: 1/(1+np.exp(-z))\n",
    "logloss = lambda p,y: -np.mean(y*np.log(p+1e-12)+(1-y)*np.log(1-p+1e-12))\n",
    "\n",
    "def f(w,X,y):  return logloss(sigmoid(X@w), y)\n",
    "def g(w,X,y):  return X.T @ (sigmoid(X@w)-y) / len(y)\n",
    "def H(w,X):    p = sigmoid(X@w).ravel(); S = p*(1-p); return (X.T*S)@X / len(S)\n",
    "def acc(w,X,y):return ((sigmoid(X@w)>=0.5).astype(int)==y).mean()\n",
    "\n",
    "# ---------- Newton variants (small ridge for invertibility) ---------------------\n",
    "def newton(w0,X,y,maxit=50,lam=1e-4,tol=1e-6):\n",
    "    I, w, hist = np.eye(len(w0)), w0.copy(), []\n",
    "    for _ in range(maxit):\n",
    "        grad = g(w,X,y)\n",
    "        if norm(grad)<tol: break\n",
    "        w -= solve(H(w,X)+lam*I, grad)\n",
    "        hist.append(f(w,X,y))\n",
    "    return w, np.array(hist)\n",
    "\n",
    "def lazy_newton(w0,X,y,k=5,maxit=50,lam=1e-4,tol=1e-6):\n",
    "    I, w, hist = np.eye(len(w0)), w0.copy(), []\n",
    "    Hk = H(w,X)+lam*I\n",
    "    for it in range(maxit):\n",
    "        grad=g(w,X,y)\n",
    "        if norm(grad)<tol: break\n",
    "        if it%k==0: Hk = H(w,X)+lam*I\n",
    "        w -= solve(Hk,grad)\n",
    "        hist.append(f(w,X,y))\n",
    "    return w, np.array(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8452fa6a-7795-467e-b360-5fc3c0f48bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 10, 50]          # 30→30, 30→300, 30→1500 raw feats\n",
    "table   = []\n",
    "\n",
    "for k in factors:\n",
    "    X_tr,X_te,y_tr,y_te = make_train_test(k)\n",
    "    d   = X_tr.shape[1]\n",
    "    w0  = np.zeros((d,1))\n",
    "\n",
    "    # -- Newton -----------------------------------------------------------------\n",
    "    t0=time.perf_counter(); wN,lN = newton(w0,X_tr,y_tr);     tN=time.perf_counter()-t0\n",
    "\n",
    "    # -- Lazy-Newton ------------------------------------------------------------\n",
    "    t0=time.perf_counter(); wL,lL = lazy_newton(w0,X_tr,y_tr);tL=time.perf_counter()-t0\n",
    "\n",
    "    # -- BFGS -------------------------------------------------------------------\n",
    "    nll   = lambda v: f(v[:,None],X_tr,y_tr)\n",
    "    gradl = lambda v: g(v[:,None],X_tr,y_tr).ravel()\n",
    "    t0=time.perf_counter()\n",
    "    wB   = minimize(nll,w0.ravel(),jac=gradl,method='BFGS',\n",
    "                    options={'gtol':1e-6,'disp':False}).x[:,None]\n",
    "    tB=time.perf_counter()-t0\n",
    "\n",
    "    # -- L-BFGS (maxcor=10) -----------------------------------------------------\n",
    "    t0=time.perf_counter()\n",
    "    wLB  = minimize(nll,w0.ravel(),jac=gradl,method='L-BFGS-B',\n",
    "                    options={'gtol':1e-6,'maxcor':10,'disp':False}).x[:,None]\n",
    "    tLB=time.perf_counter()-t0\n",
    "\n",
    "    # -- store results ----------------------------------------------------------\n",
    "    table.append([d-1,                           # raw features\n",
    "                  (\"Newton\",     tN,  len(lN),  acc(wN,X_te,y_te)),\n",
    "                  (\"Lazy-Newton\",tL,  len(lL),  acc(wL,X_te,y_te)),\n",
    "                  (\"BFGS\",       tB,           None, acc(wB,X_te,y_te)),\n",
    "                  (\"L-BFGS\",     tLB,          None, acc(wLB,X_te,y_te))])\n",
    "\n",
    "    # loss-curve plot (Newton variants only)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(lN,label=\"Newton\"); plt.plot(lL,label=\"Lazy (k=5)\")\n",
    "    plt.title(f\"{d-1} raw feats\"); plt.xlabel(\"iter\"); plt.ylabel(\"log-loss\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- print summary table -------------------------------------------------\n",
    "print(f\"{'raw d':>6} | {'method':<11} | {'time(s)':>7} | {'iters':>5} | {'test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2eac7d-8cc2-4b01-9771-854141cebad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid  = lambda z: 1 / (1 + np.exp(-z))\n",
    "logloss  = lambda p, y: -np.mean(y*np.log(p+1e-12) + (1-y)*np.log(1-p+1e-12))\n",
    "\n",
    "def f(w, X, y):\n",
    "    return logloss(sigmoid(X @ w), y)\n",
    "\n",
    "def g(w, X, y):\n",
    "    p = sigmoid(X @ w)\n",
    "    return X.T @ (p - y) / len(y)\n",
    "\n",
    "def H(w, X):\n",
    "    p = sigmoid(X @ w).ravel()\n",
    "    S = p * (1 - p)\n",
    "    return (X.T * S) @ X / len(S)\n",
    "\n",
    "def accuracy(w, X, y):\n",
    "    return ((sigmoid(X @ w) >= 0.5).astype(int) == y).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c491d07-25ed-40ef-9bc4-11abec4e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(w0, X, y, maxit=50, tol=1e-6, lam=1e-4):\n",
    "    w = w0.copy(); I = np.eye(w.size); hist=[]\n",
    "    for _ in range(maxit):\n",
    "        grad = g(w, X, y)\n",
    "        if norm(grad) < tol: break\n",
    "        w -= solve(H(w, X) + lam*I, grad)   # ridge for invertibility\n",
    "        hist.append(f(w, X, y))\n",
    "    return w, np.array(hist)\n",
    "\n",
    "def lazy_newton(w0, X, y, k=5, maxit=50, tol=1e-6, lam=1e-4):\n",
    "    w = w0.copy(); I = np.eye(w.size); hist=[]\n",
    "    Hk = H(w, X) + lam*I\n",
    "    for it in range(maxit):\n",
    "        grad = g(w, X, y)\n",
    "        if norm(grad) < tol: break\n",
    "        if it % k == 0:\n",
    "            Hk = H(w, X) + lam*I\n",
    "        w -= solve(Hk, grad)\n",
    "        hist.append(f(w, X, y))\n",
    "    return w, np.array(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2597fc2-4ce0-479c-989c-5425efef98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method       | time (s) | train |  test\n",
      "----------------------------------------\n",
      "Newton       |    22.70 | 1.000 | 0.956\n",
      "Lazy-Newton  |    19.18 | 1.000 | 0.956\n",
      "BFGS         |    65.77 | 1.000 | 0.956\n",
      "L-BFGS       |     0.10 | 1.000 | 0.956\n"
     ]
    }
   ],
   "source": [
    "k = 100   # 30 → 3 000 raw features (3 001 inc. bias)\n",
    "X_tr, X_te, y_tr, y_te = make_train_test(k)\n",
    "d  = X_tr.shape[1]\n",
    "w0 = np.zeros((d, 1))\n",
    "\n",
    "bench = []\n",
    "\n",
    "# Newton\n",
    "t0 = time.perf_counter()\n",
    "wN, _ = newton(w0, X_tr, y_tr)\n",
    "bench.append((\"Newton\", time.perf_counter()-t0,\n",
    "              accuracy(wN,X_tr,y_tr), accuracy(wN,X_te,y_te)))\n",
    "\n",
    "# Lazy-Newton\n",
    "t0 = time.perf_counter()\n",
    "wL, _ = lazy_newton(w0, X_tr, y_tr, k=5)\n",
    "bench.append((\"Lazy-Newton\", time.perf_counter()-t0,\n",
    "              accuracy(wL,X_tr,y_tr), accuracy(wL,X_te,y_te)))\n",
    "\n",
    "# BFGS via SciPy\n",
    "def nll(vec):      return f(vec[:,None], X_tr, y_tr)\n",
    "def grad_nll(vec): return g(vec[:,None], X_tr, y_tr).ravel()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "bfgs = minimize(nll, w0.ravel(), jac=grad_nll, method='BFGS',\n",
    "                options={'gtol':1e-6, 'disp':False})\n",
    "wB = bfgs.x[:,None]\n",
    "bench.append((\"BFGS\", time.perf_counter()-t0,\n",
    "              accuracy(wB,X_tr,y_tr), accuracy(wB,X_te,y_te)))\n",
    "\n",
    "# L-BFGS-B\n",
    "t0 = time.perf_counter()\n",
    "lb  = minimize(nll, w0.ravel(), jac=grad_nll, method='L-BFGS-B',\n",
    "               options={'gtol':1e-6, 'maxcor':10, 'disp':False})\n",
    "wLB = lb.x[:,None]\n",
    "bench.append((\"L-BFGS\", time.perf_counter()-t0,\n",
    "              accuracy(wLB,X_tr,y_tr), accuracy(wLB,X_te,y_te)))\n",
    "\n",
    "# ---- pretty print table --------------------------------------\n",
    "print(f\"{'Method':<12} | {'time (s)':>8} | {'train':>5} | {'test':>5}\")\n",
    "print('-'*40)\n",
    "for name,t,tr,te in bench:\n",
    "    print(f\"{name:<12} | {t:8.2f} | {tr:.3f} | {te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee4150-154e-46a1-85c9-bb8ff938ef40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
